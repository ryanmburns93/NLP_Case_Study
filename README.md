# NLP Case Study

## Abstract
Recent advancements in natural language processing techniques enable unsupervised learning of unlabeled corpora to construct models exhibiting state-of-the-art performance. Corporations with a customer-centric service-based model can leverage these techniques to build models pretrained on client facing materials. These models can be fine-tuned to deliver novel client-facing products or achieve greater efficiency on client-centric internal processes. In this paper, I pretrain bidirectional encoder representations from transformers (BERT) on a set of research document products. The transformers are embedded into a traditional categorization model to classify client service requests for routing to the appropriate consultants for the request topic. The models show modest improvements in performance compared to a baseline BERT model without pretraining. This project operates as a case study on the common obstacles faced in a corporate, low-resource environment, and the results are a proof-of-concept for future investment in unsupervised learning of natural language in service-based business models.

## Code Files
1. Data preprocessing
2. BERT Model Pretraining
3. BERT Model Finetuning

## Future Work
